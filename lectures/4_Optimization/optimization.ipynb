{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy1: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train shape -> 3073 x 50000 (각 column이 example)\n",
    "# Y_train shape -> 50000칸 array\n",
    "# L = loss function\n",
    "import numpy as np\n",
    "\n",
    "best_loss = float(\"inf\")\t# python에서 가장 큰 실수값\n",
    "\n",
    "for num in range(1000):\n",
    "\tW = np.random.randn(10, 3073) * 0.0001\n",
    "\tloss = L(X_train, Y_train, W)\n",
    "\tif loss < best_loss:\n",
    "\t\tbest_loss = loss\n",
    "\t\tbestW = W\n",
    "\tprint(\"In attempt %d, Loss was %f, best %f\" % (num, loss, function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy2: Random Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 3073) * 0.001\n",
    "best_loss = float(\"inf\")\n",
    "step_size = 0.0001\n",
    "\n",
    "for i in range(1000):\n",
    "\tW_try = W + np.random.rand(10, 3073) * step_size\n",
    "\tloss = L(Xtr_cols, Ytr, W_try)\n",
    "\tif loss < best_loss:\n",
    "\t\tW = W_try\n",
    "\t\tbest_loss = loss\n",
    "\tprint(\"Iter %d loss is %f\" % (i, best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent의 Vanilla version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tweights_grad = evaluate_gradient(loss_func, data, weights)\n",
    "\tweights += -step_size * weights_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tdata_batch = sample_training_data(data, 256)\t# Train data 중 random하게 256장 선택\n",
    "\tweights_grad = evaluate_gradient(loss_func, data_batch, weights)\n",
    "\tweights += -step_size * weights_grad"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
